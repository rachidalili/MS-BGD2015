{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposition d'algorithme pour le choix des PFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Ce notebook est prévu pour être lancé sur le master d'un cluster spark, par exemple avec la commande : **   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" ./spark-1.5.1-bin-hadoop2.6/bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ou en local, par exemple avec par exemple la commande (pour python3) : **   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PYSPARK_PYTHON=/usr/bin/python PYSPARK_DRIVER_PYTHON=/usr/bin/ipython PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" ~/Spark/spark-1.5.1-bin-hadoop2.6/bin/pyspark --master=\"local[4]\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un principe itératif:\n",
    "1. on regarde pour chacune son ou ses projets préférés, puis on détermine tous les groupes valides que l'on peut faire à partir de là.\n",
    "2. si aucune solution n'est retournée, on baisse d'un point les notes maximales (on passe toutes les notes 50 à 49 par exemple) et on recommence au point 1.\n",
    "3. si au moins une solution est retournée, c'est fini. Il reste à choisir la solution en bonne intelligence...\n",
    "\n",
    "De cette manière on cherche à satisfaire chacun au mieux, si ce n'est pas possible on *baisse un peu la barre* pour tout le monde et on recommence. Je trouve que c'est mieux que de chercher à maximiser une fonction globale qui pourrait satisfaire beaucoup la majorité et laisser quelques très déçus.\n",
    "\n",
    "Par ailleurs, cet algorithme propose l'ensemble des (meilleures) possibilités, il n'y a pas d'aléatoire. L'implémentation est (assez) simple, mais un peu longue à tourner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nouvelle implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principe de la nouvelle implémentation :  \n",
    "- on part d'une matrice M de booleans (Mij == True <=> le projet_j fait partie des projets préférés de la personne_i) ;\n",
    "- on peut tester si la matrice représente une configuration acceptable (i.e. si chaque personne n'a qu'un seul projet affecté et que les groupes sont tous de 4 personnes sauf un groupe de 3 personnes)\n",
    "- si la matrice n'est pas acceptable, alors on tente de la rendre acceptable en la transformant (on transforme un ou plusieurs True en False)\n",
    "- on continue en testant toutes les possibilités jusqu'à obtenir soit une matrice acceptable, soit une matrice non acceptable (e.g. avec une personne affectée à 0 projets).\n",
    "\n",
    "L'ensemble des possibilités étant très large, on tente de faire des transformation intelligentes et de détecter au plus tôt les configurations qui ne peuvent aboutir à une matrice acceptable, quelque soit les futures transformations appliquées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changement du code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai mis en 'dur' la taille des groupes (un groupe de 3 et sept groupes de 4). Ce qui accélère les opérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suite au mail de Stephan..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai ajouté la possibilité d'avoir trois groupes de 5 sur les projets BNP, SACEM, IPSEN et SFR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai enlevé les appels récursifs de la fonction algo qui pouvaient causer des stack overflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class DataError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_M(df, cut):\n",
    "    '''Transform the initial dataframe into a matrix of bolean M such as:\n",
    "    Mij = True iff project_j is one of the person_i's preferred project\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : the initial dataframe\n",
    "    cut : the value at which a project is considered to be a 'preferred' project\n",
    "    Returns:\n",
    "    --------\n",
    "    A n x p boolean Numpy array.\n",
    "    '''\n",
    "    val = df.values.copy()\n",
    "    val[val > cut] = cut\n",
    "    max_lines = val.max(axis=1)\n",
    "    M = (val.T == max_lines).T\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_and_check_M(M, projects_5):\n",
    "    '''Remove (in-place) some impossible choices (the more and the faster we can find them the better).\n",
    "    Then test for (in)validity\n",
    "    Current cleaning ideas:\n",
    "    (i) removing (setting the column to False) projects with strictly less than 3 people on it\n",
    "    (ii) removing people from projects who are full (i.e. already 4 or 5 (for authorized projs) \n",
    "         people sure on it) ;\n",
    "         may need several passes\n",
    "    Current checking for invalidity ideas:\n",
    "    (i) there exists i such that sum_i Mij == 0\n",
    "    (ii) there is a project with more than 4 people (sure), on it\n",
    "    (iii) there is more than one project with exactly 3 people on it (check on totally defined projects)\n",
    "    Current checking for validity ideas:\n",
    "    (i) sum_j Mij == 1 for all i\n",
    "    (ii) [sum_i Mij for i in 1..n] contains only 0s, 4s, 5s and at most one 3.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : numpy boolean array to be cleaned\n",
    "    projects_5 : a p Numpy boolean array with True where the project can hold a team of 5\n",
    "    Returns:\n",
    "    --------\n",
    "    -1 if invalid\n",
    "    +1 if valid\n",
    "     0 if we do not know and must continue our search\n",
    "    '''\n",
    "    # Cleaning.. \n",
    "    sum_cols = M.sum(axis=0)\n",
    "    sum_lines = M.sum(axis=1)\n",
    "    # .. (i)\n",
    "    M[:, sum_cols < 3] = False\n",
    "    # .. (ii)\n",
    "    flag = True\n",
    "    while flag:\n",
    "        pers_sure = sum_lines == 1\n",
    "        min_pers_on_projects = M[pers_sure].sum(axis=0)\n",
    "        full_projs = np.logical_and(min_pers_on_projects >= 4,\n",
    "                                    np.logical_or(np.invert(projects_5),\n",
    "                                                  min_pers_on_projects >= 5))\n",
    "        sub_M = M[np.ix_(np.invert(pers_sure), full_projs)]\n",
    "        flag = np.any(sub_M) # if there is something to change\n",
    "        #print(\"flag is {}...\".format(flag))\n",
    "        if flag:\n",
    "            M[np.ix_(np.invert(pers_sure), full_projs)] = False\n",
    "            \n",
    "    # Checking... \n",
    "    sum_lines = M.sum(axis=1)\n",
    "    sum_cols = M.sum(axis=0)\n",
    "    pers_sure = sum_lines == 1\n",
    "    min_pers_on_projects = M[pers_sure].sum(axis=0)\n",
    "    # projects already defined = projects which are the only choice of the people on it\n",
    "    # i.e. projects that will not be reduced since it would make a person project-less\n",
    "    already_defined_projects = min_pers_on_projects == sum_cols\n",
    "    # .. if invalid\n",
    "    # (i) everyone got at least one project\n",
    "    # (ii) no project exceed maximum limit\n",
    "    # (iii) no more than one project with exactly three people on it\n",
    "    if np.any(sum_lines == 0) or \\\n",
    "    np.any(np.logical_and(min_pers_on_projects > 4,\n",
    "           np.logical_or(np.invert(projects_5),\n",
    "                         min_pers_on_projects > 5))) or \\\n",
    "    sum(sum_cols[already_defined_projects] == 3) > 1:\n",
    "        return -1\n",
    "    \n",
    "    # .. if valid\n",
    "    if np.all(pers_sure) and \\\n",
    "    (not [1 for s in sum_cols if s not in [0, 4, 5, 3]]) and \\\n",
    "    (np.sum(sum_cols == 3) <= 1):\n",
    "        return 1\n",
    "    \n",
    "    # If we do not know...\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_M(M):\n",
    "    '''returns a list of copies of the matrix M with each correspond to a step towards\n",
    "    a solution\n",
    "    Best ideas so far : \n",
    "    (i) we pick the person with the fewest number of preferred projects (k > 1) and\n",
    "        create k different possible matrices\n",
    "    '''\n",
    "    sum_lines = M.sum(axis=1)\n",
    "    sum_lines[sum_lines == 1] = 32768 # arbitrary large value, must be larger than the number of projects\n",
    "    pers = np.argmin(sum_lines)\n",
    "    for j, proj in enumerate(M[pers]):\n",
    "        if proj:\n",
    "            new_M = M.copy()\n",
    "            new_M[pers] = 0\n",
    "            new_M[pers, j] = True\n",
    "            yield new_M\n",
    "    #Can we assure that the cleaned version will yield different matrices ??? \n",
    "    # -> In that case, yes since the cleaned matrices will either be different or invalid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_check_generate(M, p5_broad):\n",
    "    '''Function to be mapped over the RDD'''\n",
    "    check = clean_and_check_M(M, p5_broad.value)\n",
    "    # if the matrix is valid\n",
    "    if check == 1:\n",
    "        return [(True, M)]\n",
    "    # if the matrix may be valid but need further investigation\n",
    "    elif check == 0:\n",
    "        return [(False, m) for m in next_M(M)]\n",
    "    # if the matrix is invalid\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction principale de l'algorithme, qui explore l'arbre des possibilités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_groups(M, projects_5):\n",
    "    '''Returns a list of acceptable projects\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : an nxp Numpy boolean array representing preferred projects per persons\n",
    "    projects_5 : a p Numpy boolean array with True where the project can hold a team of 5\n",
    "    Returns:\n",
    "    --------\n",
    "    a list of valid numpy arrays \n",
    "    '''\n",
    "    sols = []\n",
    "    # We make an RDD\n",
    "    Ms = sc.parallelize([M])\n",
    "    p5_broad = sc.broadcast(projects_5)\n",
    "    # function of 1 parameter to be passed in a flatmap later\n",
    "    f = lambda m: clean_check_generate(m, p5_broad)\n",
    "    # keeps track of length of the RDD (without having to call an expensive 'count')\n",
    "    count = 1\n",
    "    while count > 0:\n",
    "        print(\" New iteration, ToCheck={}, Found={}.\".format(count, len(sols)))\n",
    "        count = 0\n",
    "        #print('We check \\n {}'.format(m))\n",
    "        NewMs = Ms.flatMap(f)\n",
    "        for i, ms in NewMs.groupByKey().collect():\n",
    "            if not i:\n",
    "                Ms = sc.parallelize(ms)\n",
    "                count += len(ms)\n",
    "            else:\n",
    "                sols = sols + ms.data\n",
    "    return sols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utile qui cappe les valeurs à une certaine valeur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def haircut(dat, n):\n",
    "    ''' Caps all numbers at n'''\n",
    "    dat_cap = dat.applymap(lambda x: min(n, x))\n",
    "    return dat_cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme complet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def algo(dat, project_5, cap=50):\n",
    "    '''Main algorithm (recursive)'''\n",
    "    # If the data is correct, this should not happen\n",
    "    # put it there to prevent infinite loop\n",
    "    sol = []\n",
    "    while not sol and cap > 0:\n",
    "        print(\"Run for cap={}...\".format(cap))\n",
    "        m = make_M(dat, cap)\n",
    "        sol = make_groups(m, projects_5)\n",
    "        cap -= 1\n",
    "    if not sol:\n",
    "        raise DataError('Bad data, no solution is possible')\n",
    "    return sol, cap   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec nos données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import et nettoyage des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except:\n",
    "    from io import StringIO\n",
    "r = requests.get('https://docs.google.com/spreadsheets/d/1hUWvO8wyEJL-_SkhgpdrwdYiDL7XQdrbTkcp21py8Lw/export?format=csv&id=1hUWvO8wyEJL-_SkhgpdrwdYiDL7XQdrbTkcp21py8Lw&gid=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat_bgd = pd.read_csv(StringIO(r.text), skiprows=1, index_col='Nom')\n",
    "dat_bgd = dat_bgd.loc[:, 'Clustaar':'Plume Labs']\n",
    "dat_bgd = dat_bgd.fillna(0)\n",
    "dat_bgd = dat_bgd.loc[[i for i in dat_bgd.index if isinstance(i, str)], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projets où un groupe de 5 personnes est authorisé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "projects_5 = pd.Series(data=np.array([False] * 15), index=dat_bgd.columns)\n",
    "projects_5['BNP'] = True\n",
    "projects_5['SACEM'] = True\n",
    "projects_5['IPSEN'] = True\n",
    "projects_5['SFR 1'] = True\n",
    "projects_5['SFR 2'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test s'il y a bien 31 personnes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 personnes ont répondu.\n",
      "Il faut ajouter 1 personne(s).\n"
     ]
    }
   ],
   "source": [
    "n, p = dat_bgd.shape\n",
    "print('{} personnes ont répondu.'.format(n))\n",
    "if n < 31:\n",
    "    print('Il faut ajouter {} personne(s).'.format(31-n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il manque Cynthia (il faut qu'il y ait 31 personnes sinon l'algo ne trouvera rien !):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, p = dat_bgd.shape\n",
    "if n < 31:\n",
    "    cv = pd.Series(name='Cynthia VARIEUX', index=dat_bgd.columns, data=[100./15] * 15)\n",
    "    dat_bgd = dat_bgd.append(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin que la somme des notes fasse 100 et qu'aucun projet ne se voit affecter plus de 50 points, on répartit :\n",
    "* les points manquants entre tous les projets ;\n",
    "* les points au dessus de 50 entre les autres projet.\n",
    "\n",
    "Plusieurs passes peuvent être nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_bad(df):\n",
    "    bad_sums = np.abs(dat_bgd.sum(axis=1) - 100) > 0.001 \n",
    "    bad_cells = dat_bgd > 50\n",
    "    return np.any(bad_sums) or np.any(bad_cells)\n",
    "\n",
    "while is_bad(dat_bgd):\n",
    "    # cut à 50:\n",
    "    bad_cells = dat_bgd > 50\n",
    "    dat_bgd[bad_cells] = 50\n",
    "    # repartition des points manquants (hors des cells déjà à 50)\n",
    "    pts_a_repartir = 100 - dat_bgd.sum(axis=1)\n",
    "    non_max_cells = dat_bgd < 50\n",
    "    denominator = non_max_cells.sum(axis=1)\n",
    "    to_add = (non_max_cells.T * np.array(pts_a_repartir / denominator)).T\n",
    "    dat_bgd = dat_bgd + to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécution de l'algorithme (attention, ça peut être long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run for cap=50...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=49...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=48...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=47...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=46...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=45...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=44...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=43...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=42...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=41...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=40...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=39...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=38...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=37...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=36...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=35...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=34...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=33...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=32...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=31...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      "Run for cap=30...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      " New iteration, ToCheck=2, Found=0.\n",
      " New iteration, ToCheck=4, Found=0.\n",
      " New iteration, ToCheck=8, Found=0.\n",
      " New iteration, ToCheck=12, Found=0.\n",
      " New iteration, ToCheck=20, Found=0.\n",
      " New iteration, ToCheck=39, Found=0.\n",
      " New iteration, ToCheck=65, Found=0.\n",
      " New iteration, ToCheck=106, Found=0.\n",
      " New iteration, ToCheck=135, Found=0.\n",
      " New iteration, ToCheck=125, Found=0.\n",
      " New iteration, ToCheck=102, Found=0.\n",
      " New iteration, ToCheck=79, Found=0.\n",
      " New iteration, ToCheck=59, Found=0.\n",
      " New iteration, ToCheck=20, Found=0.\n",
      "Run for cap=29...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      " New iteration, ToCheck=2, Found=0.\n",
      " New iteration, ToCheck=4, Found=0.\n",
      " New iteration, ToCheck=8, Found=0.\n",
      " New iteration, ToCheck=12, Found=0.\n",
      " New iteration, ToCheck=20, Found=0.\n",
      " New iteration, ToCheck=39, Found=0.\n",
      " New iteration, ToCheck=65, Found=0.\n",
      " New iteration, ToCheck=106, Found=0.\n",
      " New iteration, ToCheck=135, Found=0.\n",
      " New iteration, ToCheck=125, Found=0.\n",
      " New iteration, ToCheck=102, Found=0.\n",
      " New iteration, ToCheck=79, Found=0.\n",
      " New iteration, ToCheck=59, Found=0.\n",
      " New iteration, ToCheck=20, Found=0.\n",
      "Run for cap=28...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      " New iteration, ToCheck=2, Found=0.\n",
      " New iteration, ToCheck=4, Found=0.\n",
      " New iteration, ToCheck=8, Found=0.\n",
      " New iteration, ToCheck=14, Found=0.\n",
      " New iteration, ToCheck=22, Found=0.\n",
      " New iteration, ToCheck=38, Found=0.\n",
      " New iteration, ToCheck=66, Found=0.\n",
      " New iteration, ToCheck=129, Found=0.\n",
      " New iteration, ToCheck=223, Found=0.\n",
      " New iteration, ToCheck=352, Found=0.\n",
      " New iteration, ToCheck=583, Found=0.\n",
      " New iteration, ToCheck=977, Found=0.\n",
      " New iteration, ToCheck=1775, Found=0.\n",
      " New iteration, ToCheck=2272, Found=0.\n",
      " New iteration, ToCheck=3011, Found=0.\n",
      " New iteration, ToCheck=2325, Found=0.\n",
      " New iteration, ToCheck=1339, Found=0.\n",
      " New iteration, ToCheck=877, Found=0.\n",
      "Run for cap=27...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      " New iteration, ToCheck=2, Found=0.\n",
      " New iteration, ToCheck=4, Found=0.\n",
      " New iteration, ToCheck=8, Found=0.\n",
      " New iteration, ToCheck=16, Found=0.\n",
      " New iteration, ToCheck=30, Found=0.\n",
      " New iteration, ToCheck=46, Found=0.\n",
      " New iteration, ToCheck=80, Found=0.\n",
      " New iteration, ToCheck=142, Found=0.\n",
      " New iteration, ToCheck=265, Found=0.\n",
      " New iteration, ToCheck=469, Found=0.\n",
      " New iteration, ToCheck=781, Found=0.\n",
      " New iteration, ToCheck=1351, Found=0.\n",
      " New iteration, ToCheck=2070, Found=0.\n",
      " New iteration, ToCheck=3356, Found=0.\n",
      " New iteration, ToCheck=4407, Found=0.\n",
      " New iteration, ToCheck=5214, Found=0.\n",
      " New iteration, ToCheck=4813, Found=0.\n",
      " New iteration, ToCheck=2952, Found=0.\n",
      " New iteration, ToCheck=1960, Found=0.\n",
      "Run for cap=26...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      " New iteration, ToCheck=2, Found=0.\n",
      " New iteration, ToCheck=4, Found=0.\n",
      " New iteration, ToCheck=8, Found=0.\n",
      " New iteration, ToCheck=16, Found=0.\n",
      " New iteration, ToCheck=32, Found=0.\n",
      " New iteration, ToCheck=58, Found=0.\n",
      " New iteration, ToCheck=98, Found=0.\n",
      " New iteration, ToCheck=170, Found=0.\n",
      " New iteration, ToCheck=302, Found=0.\n",
      " New iteration, ToCheck=564, Found=0.\n",
      " New iteration, ToCheck=976, Found=0.\n",
      " New iteration, ToCheck=1626, Found=0.\n",
      " New iteration, ToCheck=2708, Found=0.\n",
      " New iteration, ToCheck=3992, Found=0.\n",
      " New iteration, ToCheck=5792, Found=0.\n",
      " New iteration, ToCheck=7203, Found=0.\n",
      " New iteration, ToCheck=7966, Found=0.\n",
      " New iteration, ToCheck=7672, Found=0.\n",
      " New iteration, ToCheck=4839, Found=0.\n",
      " New iteration, ToCheck=3249, Found=0.\n",
      "Run for cap=25...\n",
      " New iteration, ToCheck=1, Found=0.\n",
      " New iteration, ToCheck=2, Found=0.\n",
      " New iteration, ToCheck=4, Found=0.\n",
      " New iteration, ToCheck=8, Found=0.\n",
      " New iteration, ToCheck=16, Found=0.\n",
      " New iteration, ToCheck=32, Found=0.\n",
      " New iteration, ToCheck=64, Found=0.\n",
      " New iteration, ToCheck=128, Found=0.\n",
      " New iteration, ToCheck=244, Found=0.\n",
      " New iteration, ToCheck=460, Found=0.\n",
      " New iteration, ToCheck=920, Found=0.\n",
      " New iteration, ToCheck=1840, Found=0.\n",
      " New iteration, ToCheck=3680, Found=0.\n",
      " New iteration, ToCheck=7344, Found=0.\n",
      " New iteration, ToCheck=14611, Found=0.\n",
      " New iteration, ToCheck=29099, Found=0.\n",
      " New iteration, ToCheck=57373, Found=0.\n",
      " New iteration, ToCheck=112465, Found=0.\n",
      " New iteration, ToCheck=224501, Found=0.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 351.0 failed 1 times, most recent failure: Lost task 0.0 in stage 351.0 (TID 1404, localhost): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-6f39545f719f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat_bgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprojects_5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'enlapsed time = {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-54875c1035b5>\u001b[0m in \u001b[0;36malgo\u001b[1;34m(dat, project_5, cap)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Run for cap={}...\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_M\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0msol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprojects_5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mcap\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-176200787cfa>\u001b[0m in \u001b[0;36mmake_groups\u001b[1;34m(M, projects_5)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#print('We check \\n {}'.format(m))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mNewMs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mNewMs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mMs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/Spark/spark-1.5.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \"\"\"\n\u001b[0;32m    772\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/Spark/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/Spark/spark-1.5.1-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/Spark/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 351.0 failed 1 times, most recent failure: Lost task 0.0 in stage 351.0 (TID 1404, localhost): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.datetime.now()\n",
    "sols, cut = algo(dat_bgd, projects_5)\n",
    "t1 = datetime.datetime.now()\n",
    "print('enlapsed time = {}'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_M_df(M, df):\n",
    "    sol_df = pd.DataFrame(data=M, columns=df.columns, index=df.index)\n",
    "    sol_serie = sol_df.apply(lambda s: s.argmax(), axis=1)\n",
    "    return sol_serie\n",
    "\n",
    "def convert_sols_df(sols, df):\n",
    "    sol = pd.DataFrame(data=[convert_M_df(M, df) for M in sols])\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_tab = convert_sols_df(sols, dat_bgd)\n",
    "#final_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_tab.to_csv('sols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_row(row):\n",
    "    d = {proj: [] for proj in dat_bgd.columns}\n",
    "    for name, proj in zip(row.index, row):\n",
    "        d[proj].append(name)\n",
    "    return d\n",
    "\n",
    "def extract_teams(final_tab):\n",
    "    for row in final_tab.index:\n",
    "        print(\"\\nSolution {}:\".format(row))\n",
    "        d = from_row(final_tab.loc[row])\n",
    "        for proj in dat_bgd.columns:\n",
    "            print(\"Projet {:15s}: {}\".format(proj, ', '.join(d[proj])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract_teams(final_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write all the above information into a csv file (to be imported in a spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('output.csv', 'w') as f:\n",
    "    f.write('Output de l\\'algorithme (base de discussion).\\n'\n",
    "            'Date de génération {}\\n.'.format(t0))\n",
    "    f.write('Le cut a été effectué à {} points.\\n'.format(cut))\n",
    "    f.write('Source : https://github.com/rachidalili/MS-BGD2015/blob/master/guillaume-mohr/algoPFR/AlgoPFR-New-SPARK.ipynb\\n'\n",
    "            'Note 1 : les personnes n\\'ayant pas rempli le tableau sont ajoutées '\n",
    "            'et sont supposées affecter un poid égal à chaque projet.\\n'\n",
    "            'Note 2 : les personnes n\\'ayant pas affecté 100 points sont supposées '\n",
    "            'affecter leur restant de points de manière équitable entre tous les '\n",
    "            'projets (en respectant la limite de 50 points max par projet).\\n'\n",
    "            '\"Note 3 : un seul groupe de 3 personnes permis, groupes de 5 personnes permis sur les'\n",
    "            ' projets IPSEN / SACEM / SFR 1 / SFR 2 / BNP\"\\n\\n'\n",
    "            'TABLEAU RECAPITULATIF DES SOLUTIONS\\n')\n",
    "    f.write(final_tab.to_csv(None))\n",
    "    f.write('\\n\\nDETAILS DES SOLUTIONS')\n",
    "    for row in final_tab.index:\n",
    "        f.write('\\nSOLUTION N°{}\\n'.format(row))\n",
    "        f.write('Projet,Equipe\\n')\n",
    "        d = from_row(final_tab.loc[row])\n",
    "        for proj in dat_bgd.columns:\n",
    "            f.write('{},{}\\n'.format(proj, ','.join(d[proj])))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
